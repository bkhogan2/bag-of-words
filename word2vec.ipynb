{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n",
      "50000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter='\\t',quoting=3)\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "\n",
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "\n",
    "print(train.size)\n",
    "print(test.size)\n",
    "print(unlabeled_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if(remove_stopwords):\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if w not in stops]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print (\"Parsing sentences from unlabeled set\")\n",
    "\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-10 09:50:20,532 : INFO : collecting all words and their counts\n",
      "2019-07-10 09:50:20,533 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-07-10 09:50:20,642 : INFO : PROGRESS: at sentence #10000, processed 227352 words, keeping 18345 word types\n",
      "2019-07-10 09:50:20,709 : INFO : PROGRESS: at sentence #20000, processed 455015 words, keeping 26112 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-10 09:50:20,804 : INFO : PROGRESS: at sentence #30000, processed 675833 words, keeping 31668 word types\n",
      "2019-07-10 09:50:20,875 : INFO : PROGRESS: at sentence #40000, processed 903705 words, keeping 36428 word types\n",
      "2019-07-10 09:50:20,938 : INFO : PROGRESS: at sentence #50000, processed 1124382 words, keeping 40273 word types\n",
      "2019-07-10 09:50:20,999 : INFO : PROGRESS: at sentence #60000, processed 1347279 words, keeping 43577 word types\n",
      "2019-07-10 09:50:21,062 : INFO : PROGRESS: at sentence #70000, processed 1571925 words, keeping 46571 word types\n",
      "2019-07-10 09:50:21,122 : INFO : PROGRESS: at sentence #80000, processed 1792766 words, keeping 49300 word types\n",
      "2019-07-10 09:50:21,182 : INFO : PROGRESS: at sentence #90000, processed 2018456 words, keeping 52045 word types\n",
      "2019-07-10 09:50:21,241 : INFO : PROGRESS: at sentence #100000, processed 2241761 words, keeping 54429 word types\n",
      "2019-07-10 09:50:21,300 : INFO : PROGRESS: at sentence #110000, processed 2462827 words, keeping 56584 word types\n",
      "2019-07-10 09:50:21,363 : INFO : PROGRESS: at sentence #120000, processed 2686480 words, keeping 58916 word types\n",
      "2019-07-10 09:50:21,426 : INFO : PROGRESS: at sentence #130000, processed 2913619 words, keeping 60956 word types\n",
      "2019-07-10 09:50:21,485 : INFO : PROGRESS: at sentence #140000, processed 3127719 words, keeping 62674 word types\n",
      "2019-07-10 09:50:21,548 : INFO : PROGRESS: at sentence #150000, processed 3354852 words, keeping 64633 word types\n",
      "2019-07-10 09:50:21,610 : INFO : PROGRESS: at sentence #160000, processed 3579114 words, keeping 66441 word types\n",
      "2019-07-10 09:50:21,696 : INFO : PROGRESS: at sentence #170000, processed 3803821 words, keeping 68114 word types\n",
      "2019-07-10 09:50:21,754 : INFO : PROGRESS: at sentence #180000, processed 4025758 words, keeping 69757 word types\n",
      "2019-07-10 09:50:21,824 : INFO : PROGRESS: at sentence #190000, processed 4252519 words, keeping 71315 word types\n",
      "2019-07-10 09:50:21,886 : INFO : PROGRESS: at sentence #200000, processed 4478145 words, keeping 72817 word types\n",
      "2019-07-10 09:50:21,947 : INFO : PROGRESS: at sentence #210000, processed 4700981 words, keeping 74334 word types\n",
      "2019-07-10 09:50:22,004 : INFO : PROGRESS: at sentence #220000, processed 4927438 words, keeping 75848 word types\n",
      "2019-07-10 09:50:22,064 : INFO : PROGRESS: at sentence #230000, processed 5151531 words, keeping 77325 word types\n",
      "2019-07-10 09:50:22,127 : INFO : PROGRESS: at sentence #240000, processed 5380566 words, keeping 78723 word types\n",
      "2019-07-10 09:50:22,188 : INFO : PROGRESS: at sentence #250000, processed 5596061 words, keeping 80093 word types\n",
      "2019-07-10 09:50:22,313 : INFO : PROGRESS: at sentence #260000, processed 5817596 words, keeping 81390 word types\n",
      "2019-07-10 09:50:22,517 : INFO : PROGRESS: at sentence #270000, processed 6040311 words, keeping 82876 word types\n",
      "2019-07-10 09:50:22,609 : INFO : PROGRESS: at sentence #280000, processed 6267726 words, keeping 84650 word types\n",
      "2019-07-10 09:50:22,690 : INFO : PROGRESS: at sentence #290000, processed 6492340 words, keeping 86344 word types\n",
      "2019-07-10 09:50:22,786 : INFO : PROGRESS: at sentence #300000, processed 6718444 words, keeping 87876 word types\n",
      "2019-07-10 09:50:22,858 : INFO : PROGRESS: at sentence #310000, processed 6945475 words, keeping 89409 word types\n",
      "2019-07-10 09:50:22,947 : INFO : PROGRESS: at sentence #320000, processed 7171788 words, keeping 90932 word types\n",
      "2019-07-10 09:50:23,045 : INFO : PROGRESS: at sentence #330000, processed 7394895 words, keeping 92356 word types\n",
      "2019-07-10 09:50:23,225 : INFO : PROGRESS: at sentence #340000, processed 7626062 words, keeping 93801 word types\n",
      "2019-07-10 09:50:23,293 : INFO : PROGRESS: at sentence #350000, processed 7850876 words, keeping 95113 word types\n",
      "2019-07-10 09:50:23,365 : INFO : PROGRESS: at sentence #360000, processed 8073081 words, keeping 96469 word types\n",
      "2019-07-10 09:50:23,511 : INFO : PROGRESS: at sentence #370000, processed 8301799 words, keeping 97787 word types\n",
      "2019-07-10 09:50:23,721 : INFO : PROGRESS: at sentence #380000, processed 8528460 words, keeping 99141 word types\n",
      "2019-07-10 09:50:23,911 : INFO : PROGRESS: at sentence #390000, processed 8759771 words, keeping 100369 word types\n",
      "2019-07-10 09:50:24,137 : INFO : PROGRESS: at sentence #400000, processed 8984054 words, keeping 101508 word types\n",
      "2019-07-10 09:50:24,515 : INFO : PROGRESS: at sentence #410000, processed 9206876 words, keeping 102633 word types\n",
      "2019-07-10 09:50:24,715 : INFO : PROGRESS: at sentence #420000, processed 9429453 words, keeping 103804 word types\n",
      "2019-07-10 09:50:24,868 : INFO : PROGRESS: at sentence #430000, processed 9658616 words, keeping 104984 word types\n",
      "2019-07-10 09:50:25,025 : INFO : PROGRESS: at sentence #440000, processed 9886995 words, keeping 106151 word types\n",
      "2019-07-10 09:50:25,127 : INFO : PROGRESS: at sentence #450000, processed 10112087 words, keeping 107455 word types\n",
      "2019-07-10 09:50:25,245 : INFO : PROGRESS: at sentence #460000, processed 10346679 words, keeping 108660 word types\n",
      "2019-07-10 09:50:25,346 : INFO : PROGRESS: at sentence #470000, processed 10576314 words, keeping 109672 word types\n",
      "2019-07-10 09:50:25,452 : INFO : PROGRESS: at sentence #480000, processed 10798286 words, keeping 110740 word types\n",
      "2019-07-10 09:50:25,514 : INFO : PROGRESS: at sentence #490000, processed 11026646 words, keeping 111911 word types\n",
      "2019-07-10 09:50:25,675 : INFO : PROGRESS: at sentence #500000, processed 11249880 words, keeping 112941 word types\n",
      "2019-07-10 09:50:25,910 : INFO : PROGRESS: at sentence #510000, processed 11476811 words, keeping 114039 word types\n",
      "2019-07-10 09:50:26,137 : INFO : PROGRESS: at sentence #520000, processed 11701657 words, keeping 115078 word types\n",
      "2019-07-10 09:50:26,220 : INFO : PROGRESS: at sentence #530000, processed 11927584 words, keeping 116023 word types\n",
      "2019-07-10 09:50:26,282 : INFO : PROGRESS: at sentence #540000, processed 12153876 words, keeping 117045 word types\n",
      "2019-07-10 09:50:26,348 : INFO : PROGRESS: at sentence #550000, processed 12380982 words, keeping 118048 word types\n",
      "2019-07-10 09:50:26,411 : INFO : PROGRESS: at sentence #560000, processed 12603774 words, keeping 119033 word types\n",
      "2019-07-10 09:50:26,477 : INFO : PROGRESS: at sentence #570000, processed 12834311 words, keeping 119968 word types\n",
      "2019-07-10 09:50:26,563 : INFO : PROGRESS: at sentence #580000, processed 13057178 words, keeping 120984 word types\n",
      "2019-07-10 09:50:26,847 : INFO : PROGRESS: at sentence #590000, processed 13284207 words, keeping 121969 word types\n",
      "2019-07-10 09:50:27,020 : INFO : PROGRESS: at sentence #600000, processed 13508044 words, keeping 122832 word types\n",
      "2019-07-10 09:50:27,107 : INFO : PROGRESS: at sentence #610000, processed 13730758 words, keeping 123848 word types\n",
      "2019-07-10 09:50:27,200 : INFO : PROGRESS: at sentence #620000, processed 13958572 words, keeping 124738 word types\n",
      "2019-07-10 09:50:27,297 : INFO : PROGRESS: at sentence #630000, processed 14184327 words, keeping 125647 word types\n",
      "2019-07-10 09:50:27,577 : INFO : PROGRESS: at sentence #640000, processed 14406643 words, keeping 126573 word types\n",
      "2019-07-10 09:50:27,721 : INFO : PROGRESS: at sentence #650000, processed 14634081 words, keeping 127471 word types\n",
      "2019-07-10 09:50:27,916 : INFO : PROGRESS: at sentence #660000, processed 14858376 words, keeping 128341 word types\n",
      "2019-07-10 09:50:27,987 : INFO : PROGRESS: at sentence #670000, processed 15083322 words, keeping 129156 word types\n",
      "2019-07-10 09:50:28,068 : INFO : PROGRESS: at sentence #680000, processed 15309753 words, keeping 129991 word types\n",
      "2019-07-10 09:50:28,139 : INFO : PROGRESS: at sentence #690000, processed 15533492 words, keeping 130889 word types\n",
      "2019-07-10 09:50:28,219 : INFO : PROGRESS: at sentence #700000, processed 15763661 words, keeping 131811 word types\n",
      "2019-07-10 09:50:28,285 : INFO : PROGRESS: at sentence #710000, processed 15988196 words, keeping 132582 word types\n",
      "2019-07-10 09:50:28,358 : INFO : PROGRESS: at sentence #720000, processed 16214984 words, keeping 133322 word types\n",
      "2019-07-10 09:50:28,424 : INFO : PROGRESS: at sentence #730000, processed 16442803 words, keeping 134168 word types\n",
      "2019-07-10 09:50:28,498 : INFO : PROGRESS: at sentence #740000, processed 16665254 words, keeping 134971 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-10 09:50:28,563 : INFO : PROGRESS: at sentence #750000, processed 16885099 words, keeping 135708 word types\n",
      "2019-07-10 09:50:28,633 : INFO : PROGRESS: at sentence #760000, processed 17105901 words, keeping 136444 word types\n",
      "2019-07-10 09:50:28,724 : INFO : PROGRESS: at sentence #770000, processed 17334591 words, keeping 137324 word types\n",
      "2019-07-10 09:50:29,060 : INFO : PROGRESS: at sentence #780000, processed 17566230 words, keeping 138131 word types\n",
      "2019-07-10 09:50:29,310 : INFO : PROGRESS: at sentence #790000, processed 17794892 words, keeping 138912 word types\n",
      "2019-07-10 09:50:29,465 : INFO : collected 139407 word types from a corpus of 17918782 raw words and 795538 sentences\n",
      "2019-07-10 09:50:29,470 : INFO : Loading a fresh vocabulary\n",
      "2019-07-10 09:50:29,798 : INFO : effective_min_count=40 retains 16612 unique words (11% of original 139407, drops 122795)\n",
      "2019-07-10 09:50:29,799 : INFO : effective_min_count=40 leaves 17306744 word corpus (96% of original 17918782, drops 612038)\n",
      "2019-07-10 09:50:29,867 : INFO : deleting the raw counts dictionary of 139407 items\n",
      "2019-07-10 09:50:29,872 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2019-07-10 09:50:29,874 : INFO : downsampling leaves estimated 12771179 word corpus (73.8% of prior 17306744)\n",
      "2019-07-10 09:50:29,946 : INFO : estimated required memory for 16612 words and 300 dimensions: 48174800 bytes\n",
      "2019-07-10 09:50:29,947 : INFO : resetting layer weights\n",
      "2019-07-10 09:50:30,482 : INFO : training model with 4 workers on 16612 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-07-10 09:50:31,493 : INFO : EPOCH 1 - PROGRESS: at 4.40% examples, 561183 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:32,507 : INFO : EPOCH 1 - PROGRESS: at 8.06% examples, 508019 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:33,529 : INFO : EPOCH 1 - PROGRESS: at 11.01% examples, 461351 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:50:34,536 : INFO : EPOCH 1 - PROGRESS: at 13.44% examples, 422416 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:35,589 : INFO : EPOCH 1 - PROGRESS: at 16.83% examples, 418851 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:36,602 : INFO : EPOCH 1 - PROGRESS: at 20.37% examples, 422655 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:37,604 : INFO : EPOCH 1 - PROGRESS: at 24.40% examples, 435222 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:38,639 : INFO : EPOCH 1 - PROGRESS: at 27.08% examples, 421723 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:39,683 : INFO : EPOCH 1 - PROGRESS: at 30.01% examples, 414834 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:40,693 : INFO : EPOCH 1 - PROGRESS: at 34.27% examples, 426023 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:41,704 : INFO : EPOCH 1 - PROGRESS: at 37.14% examples, 420582 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:42,719 : INFO : EPOCH 1 - PROGRESS: at 41.15% examples, 427573 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:50:43,748 : INFO : EPOCH 1 - PROGRESS: at 44.48% examples, 426609 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:44,750 : INFO : EPOCH 1 - PROGRESS: at 47.06% examples, 420037 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:45,753 : INFO : EPOCH 1 - PROGRESS: at 51.06% examples, 426051 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:50:46,757 : INFO : EPOCH 1 - PROGRESS: at 54.12% examples, 423751 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:47,769 : INFO : EPOCH 1 - PROGRESS: at 58.07% examples, 428590 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:48,778 : INFO : EPOCH 1 - PROGRESS: at 62.45% examples, 435687 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:50:49,788 : INFO : EPOCH 1 - PROGRESS: at 66.89% examples, 442375 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:50,794 : INFO : EPOCH 1 - PROGRESS: at 71.29% examples, 448132 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:51,866 : INFO : EPOCH 1 - PROGRESS: at 73.28% examples, 437641 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:52,882 : INFO : EPOCH 1 - PROGRESS: at 77.18% examples, 440019 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:53,895 : INFO : EPOCH 1 - PROGRESS: at 81.37% examples, 443787 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:50:54,912 : INFO : EPOCH 1 - PROGRESS: at 85.25% examples, 445713 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:55,927 : INFO : EPOCH 1 - PROGRESS: at 88.52% examples, 444451 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:50:56,959 : INFO : EPOCH 1 - PROGRESS: at 90.47% examples, 436528 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:57,983 : INFO : EPOCH 1 - PROGRESS: at 93.65% examples, 435013 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:58,991 : INFO : EPOCH 1 - PROGRESS: at 98.44% examples, 441084 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:50:59,310 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-10 09:50:59,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-10 09:50:59,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-10 09:50:59,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-10 09:50:59,347 : INFO : EPOCH - 1 : training on 17918782 raw words (12772467 effective words) took 28.9s, 442614 effective words/s\n",
      "2019-07-10 09:51:00,370 : INFO : EPOCH 2 - PROGRESS: at 3.11% examples, 393787 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:01,390 : INFO : EPOCH 2 - PROGRESS: at 7.78% examples, 486738 words/s, in_qsize 8, out_qsize 1\n",
      "2019-07-10 09:51:02,429 : INFO : EPOCH 2 - PROGRESS: at 11.36% examples, 470159 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:03,439 : INFO : EPOCH 2 - PROGRESS: at 14.97% examples, 465339 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:04,457 : INFO : EPOCH 2 - PROGRESS: at 18.29% examples, 454823 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:05,466 : INFO : EPOCH 2 - PROGRESS: at 22.44% examples, 465846 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:06,493 : INFO : EPOCH 2 - PROGRESS: at 26.96% examples, 479497 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:07,499 : INFO : EPOCH 2 - PROGRESS: at 31.63% examples, 492751 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:08,516 : INFO : EPOCH 2 - PROGRESS: at 35.71% examples, 494661 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:09,519 : INFO : EPOCH 2 - PROGRESS: at 39.50% examples, 493511 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:10,525 : INFO : EPOCH 2 - PROGRESS: at 43.68% examples, 497412 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:11,532 : INFO : EPOCH 2 - PROGRESS: at 47.85% examples, 500028 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:12,581 : INFO : EPOCH 2 - PROGRESS: at 50.67% examples, 487863 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:13,612 : INFO : EPOCH 2 - PROGRESS: at 52.35% examples, 467546 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:14,614 : INFO : EPOCH 2 - PROGRESS: at 56.12% examples, 468527 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:15,620 : INFO : EPOCH 2 - PROGRESS: at 60.40% examples, 473635 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:16,632 : INFO : EPOCH 2 - PROGRESS: at 64.73% examples, 478025 words/s, in_qsize 8, out_qsize 1\n",
      "2019-07-10 09:51:17,642 : INFO : EPOCH 2 - PROGRESS: at 67.17% examples, 468755 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:18,645 : INFO : EPOCH 2 - PROGRESS: at 71.60% examples, 473881 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:19,659 : INFO : EPOCH 2 - PROGRESS: at 75.74% examples, 476182 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:20,668 : INFO : EPOCH 2 - PROGRESS: at 78.12% examples, 467967 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:21,680 : INFO : EPOCH 2 - PROGRESS: at 81.86% examples, 468107 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:22,705 : INFO : EPOCH 2 - PROGRESS: at 83.71% examples, 457642 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:23,710 : INFO : EPOCH 2 - PROGRESS: at 87.86% examples, 460692 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:24,737 : INFO : EPOCH 2 - PROGRESS: at 92.53% examples, 465600 words/s, in_qsize 8, out_qsize 1\n",
      "2019-07-10 09:51:25,752 : INFO : EPOCH 2 - PROGRESS: at 97.29% examples, 470607 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-10 09:51:26,300 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-10 09:51:26,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-10 09:51:26,324 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-10 09:51:26,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-10 09:51:26,359 : INFO : EPOCH - 2 : training on 17918782 raw words (12771129 effective words) took 27.0s, 473005 effective words/s\n",
      "2019-07-10 09:51:27,386 : INFO : EPOCH 3 - PROGRESS: at 4.56% examples, 578864 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:28,404 : INFO : EPOCH 3 - PROGRESS: at 9.28% examples, 583042 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:29,408 : INFO : EPOCH 3 - PROGRESS: at 11.99% examples, 502841 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:30,413 : INFO : EPOCH 3 - PROGRESS: at 16.40% examples, 516935 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:31,420 : INFO : EPOCH 3 - PROGRESS: at 21.04% examples, 529329 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:32,574 : INFO : EPOCH 3 - PROGRESS: at 23.77% examples, 487059 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:33,574 : INFO : EPOCH 3 - PROGRESS: at 27.24% examples, 480555 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:34,584 : INFO : EPOCH 3 - PROGRESS: at 30.06% examples, 465595 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:35,593 : INFO : EPOCH 3 - PROGRESS: at 32.92% examples, 453213 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:36,598 : INFO : EPOCH 3 - PROGRESS: at 36.15% examples, 449011 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:37,610 : INFO : EPOCH 3 - PROGRESS: at 39.66% examples, 448548 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:38,620 : INFO : EPOCH 3 - PROGRESS: at 43.85% examples, 455708 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:39,664 : INFO : EPOCH 3 - PROGRESS: at 45.58% examples, 436472 words/s, in_qsize 8, out_qsize 1\n",
      "2019-07-10 09:51:40,682 : INFO : EPOCH 3 - PROGRESS: at 47.06% examples, 418852 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:41,727 : INFO : EPOCH 3 - PROGRESS: at 50.39% examples, 418160 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:42,761 : INFO : EPOCH 3 - PROGRESS: at 53.08% examples, 412629 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:43,787 : INFO : EPOCH 3 - PROGRESS: at 55.34% examples, 405051 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:44,827 : INFO : EPOCH 3 - PROGRESS: at 56.28% examples, 388773 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:45,829 : INFO : EPOCH 3 - PROGRESS: at 59.54% examples, 390663 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:46,841 : INFO : EPOCH 3 - PROGRESS: at 63.90% examples, 398453 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:47,849 : INFO : EPOCH 3 - PROGRESS: at 67.39% examples, 400648 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:48,875 : INFO : EPOCH 3 - PROGRESS: at 70.79% examples, 401689 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:49,878 : INFO : EPOCH 3 - PROGRESS: at 74.33% examples, 403890 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:50,896 : INFO : EPOCH 3 - PROGRESS: at 77.96% examples, 405975 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:51,897 : INFO : EPOCH 3 - PROGRESS: at 81.81% examples, 409315 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:52,910 : INFO : EPOCH 3 - PROGRESS: at 86.05% examples, 414086 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:51:53,915 : INFO : EPOCH 3 - PROGRESS: at 90.97% examples, 421970 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:54,918 : INFO : EPOCH 3 - PROGRESS: at 95.13% examples, 425587 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:55,933 : INFO : EPOCH 3 - PROGRESS: at 98.65% examples, 426380 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:51:56,240 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-10 09:51:56,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-10 09:51:56,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-10 09:51:56,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-10 09:51:56,314 : INFO : EPOCH - 3 : training on 17918782 raw words (12773994 effective words) took 29.9s, 426717 effective words/s\n",
      "2019-07-10 09:51:57,368 : INFO : EPOCH 4 - PROGRESS: at 3.67% examples, 460419 words/s, in_qsize 7, out_qsize 1\n",
      "2019-07-10 09:51:58,384 : INFO : EPOCH 4 - PROGRESS: at 8.17% examples, 510352 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:51:59,396 : INFO : EPOCH 4 - PROGRESS: at 12.84% examples, 534682 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:52:00,405 : INFO : EPOCH 4 - PROGRESS: at 17.30% examples, 540308 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:01,415 : INFO : EPOCH 4 - PROGRESS: at 22.11% examples, 553290 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:02,433 : INFO : EPOCH 4 - PROGRESS: at 26.90% examples, 561192 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:03,435 : INFO : EPOCH 4 - PROGRESS: at 31.52% examples, 564151 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:52:04,444 : INFO : EPOCH 4 - PROGRESS: at 35.43% examples, 555336 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:05,447 : INFO : EPOCH 4 - PROGRESS: at 38.31% examples, 534843 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:06,448 : INFO : EPOCH 4 - PROGRESS: at 42.08% examples, 529752 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:07,472 : INFO : EPOCH 4 - PROGRESS: at 44.99% examples, 514254 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:08,475 : INFO : EPOCH 4 - PROGRESS: at 48.44% examples, 508607 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:09,482 : INFO : EPOCH 4 - PROGRESS: at 51.35% examples, 497800 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:10,542 : INFO : EPOCH 4 - PROGRESS: at 52.52% examples, 471161 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:11,542 : INFO : EPOCH 4 - PROGRESS: at 55.01% examples, 461180 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:12,548 : INFO : EPOCH 4 - PROGRESS: at 57.68% examples, 454078 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:13,557 : INFO : EPOCH 4 - PROGRESS: at 62.50% examples, 463388 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:14,562 : INFO : EPOCH 4 - PROGRESS: at 67.33% examples, 471806 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:52:15,569 : INFO : EPOCH 4 - PROGRESS: at 72.21% examples, 479636 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:52:16,590 : INFO : EPOCH 4 - PROGRESS: at 77.01% examples, 485653 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:52:17,599 : INFO : EPOCH 4 - PROGRESS: at 81.63% examples, 490411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:18,628 : INFO : EPOCH 4 - PROGRESS: at 86.27% examples, 494263 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:52:19,631 : INFO : EPOCH 4 - PROGRESS: at 91.02% examples, 499239 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:20,648 : INFO : EPOCH 4 - PROGRESS: at 95.60% examples, 502083 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:21,653 : INFO : EPOCH 4 - PROGRESS: at 99.59% examples, 502687 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:21,727 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-10 09:52:21,746 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-10 09:52:21,748 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-10 09:52:21,767 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-10 09:52:21,768 : INFO : EPOCH - 4 : training on 17918782 raw words (12772340 effective words) took 25.4s, 502442 effective words/s\n",
      "2019-07-10 09:52:22,790 : INFO : EPOCH 5 - PROGRESS: at 4.35% examples, 550636 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:52:23,814 : INFO : EPOCH 5 - PROGRESS: at 9.39% examples, 588022 words/s, in_qsize 6, out_qsize 1\n",
      "2019-07-10 09:52:24,815 : INFO : EPOCH 5 - PROGRESS: at 14.30% examples, 598251 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:25,828 : INFO : EPOCH 5 - PROGRESS: at 18.68% examples, 585587 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:26,840 : INFO : EPOCH 5 - PROGRESS: at 23.49% examples, 589487 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:52:27,864 : INFO : EPOCH 5 - PROGRESS: at 26.95% examples, 562739 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-10 09:52:28,896 : INFO : EPOCH 5 - PROGRESS: at 30.23% examples, 540027 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:29,909 : INFO : EPOCH 5 - PROGRESS: at 33.10% examples, 516484 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:30,927 : INFO : EPOCH 5 - PROGRESS: at 36.71% examples, 509552 words/s, in_qsize 8, out_qsize 1\n",
      "2019-07-10 09:52:31,943 : INFO : EPOCH 5 - PROGRESS: at 40.31% examples, 504173 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:32,945 : INFO : EPOCH 5 - PROGRESS: at 42.79% examples, 487563 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:33,974 : INFO : EPOCH 5 - PROGRESS: at 44.07% examples, 459853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:35,004 : INFO : EPOCH 5 - PROGRESS: at 47.12% examples, 453554 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:36,018 : INFO : EPOCH 5 - PROGRESS: at 50.56% examples, 452229 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:37,024 : INFO : EPOCH 5 - PROGRESS: at 54.56% examples, 455961 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:38,049 : INFO : EPOCH 5 - PROGRESS: at 58.79% examples, 460914 words/s, in_qsize 8, out_qsize 0\n",
      "2019-07-10 09:52:39,065 : INFO : EPOCH 5 - PROGRESS: at 62.73% examples, 463000 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:40,069 : INFO : EPOCH 5 - PROGRESS: at 66.68% examples, 465236 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:41,077 : INFO : EPOCH 5 - PROGRESS: at 71.55% examples, 473384 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:42,093 : INFO : EPOCH 5 - PROGRESS: at 76.36% examples, 479809 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:43,107 : INFO : EPOCH 5 - PROGRESS: at 81.26% examples, 486372 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:44,108 : INFO : EPOCH 5 - PROGRESS: at 86.21% examples, 492925 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:45,112 : INFO : EPOCH 5 - PROGRESS: at 90.97% examples, 497916 words/s, in_qsize 7, out_qsize 0\n",
      "2019-07-10 09:52:46,121 : INFO : EPOCH 5 - PROGRESS: at 95.87% examples, 502701 words/s, in_qsize 7, out_qsize 1\n",
      "2019-07-10 09:52:46,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-10 09:52:46,947 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-10 09:52:46,963 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-10 09:52:46,973 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-10 09:52:46,974 : INFO : EPOCH - 5 : training on 17918782 raw words (12770770 effective words) took 25.2s, 506938 effective words/s\n",
      "2019-07-10 09:52:46,976 : INFO : training on a 89593910 raw words (63860700 effective words) took 136.5s, 467883 effective words/s\n",
      "2019-07-10 09:52:46,980 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-07-10 09:52:47,047 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-07-10 09:52:47,048 : INFO : not storing attribute vectors_norm\n",
      "2019-07-10 09:52:47,049 : INFO : not storing attribute cum_table\n",
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-10 09:52:47,493 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                          size=num_features, min_count = min_word_count,\n",
    "                          window=context, sample=downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6686308979988098),\n",
       " ('latifah', 0.6347348690032959),\n",
       " ('bride', 0.6235207319259644),\n",
       " ('maid', 0.6224480867385864),\n",
       " ('stepmother', 0.6088650226593018),\n",
       " ('victoria', 0.6076782941818237),\n",
       " ('belle', 0.593917191028595),\n",
       " ('angela', 0.5797292590141296),\n",
       " ('mistress', 0.5797204375267029),\n",
       " ('aurora', 0.5775083303451538)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    \n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        if counter%1000 == 0:\n",
    "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        \n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhogan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review,remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
